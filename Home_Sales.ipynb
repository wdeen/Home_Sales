{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [SparkSQL] Home Sales Dataset - Key Metrics\n",
    "---\n",
    "## Step #1 - Import Relevant Dependencies & Create Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "# Initialise findspark to find and connect with the Spark cluster; useful automation to make appropriate configs to my Python environment\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "\n",
    "# Instantiate my Spark Session\n",
    "spark = SparkSession.builder.appName(\"SparkSQL\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step #2: Read `home_sales_revised.csv` into a Spark DataFrame\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the AWS S3 bucket into a DataFrame.\n",
    "from pyspark import SparkFiles\n",
    "\n",
    "# Store the URL of the AWS S3 bucket containing the CSV Dataset\n",
    "csv_url = \"https://2u-data-curriculum-team.s3.amazonaws.com/dataviz-classroom/v1.2/22-big-data/home_sales_revised.csv\"\n",
    "\n",
    "# Download the CSV file from the AWS S3 bucket and distribute it to all nodes in the Spark cluster\n",
    "spark.sparkContext.addFile(csv_url)\n",
    "\n",
    "# From this Spark job, read the CSV dataset into a Spark DataFrame that is comma separated and the first row are column headers\n",
    "home_df = spark.read.csv(SparkFiles.get(\"home_sales_revised.csv\"), sep = \",\", header = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step #3: Create a Temporary Table Called `home_sales`\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+----------+------+--------+---------+-----------+--------+------+----------+----+\n",
      "|                  id|      date|date_built| price|bedrooms|bathrooms|sqft_living|sqft_lot|floors|waterfront|view|\n",
      "+--------------------+----------+----------+------+--------+---------+-----------+--------+------+----------+----+\n",
      "|f8a53099-ba1c-47d...|2022-04-08|      2016|936923|       4|        3|       3167|   11733|     2|         1|  76|\n",
      "|7530a2d8-1ae3-451...|2021-06-13|      2013|379628|       2|        2|       2235|   14384|     1|         0|  23|\n",
      "|43de979c-0bf0-4c9...|2019-04-12|      2014|417866|       2|        2|       2127|   10575|     2|         0|   0|\n",
      "|b672c137-b88c-48b...|2019-10-16|      2016|239895|       2|        2|       1631|   11149|     2|         0|   0|\n",
      "|e0726d4d-d595-407...|2022-01-08|      2017|424418|       3|        2|       2249|   13878|     2|         0|   4|\n",
      "|5aa00529-0533-46b...|2019-01-30|      2017|218712|       2|        3|       1965|   14375|     2|         0|   7|\n",
      "|131492a1-72e2-4a8...|2020-02-08|      2017|419199|       2|        3|       2062|    8876|     2|         0|   6|\n",
      "|8d54a71b-c520-44e...|2019-07-21|      2010|323956|       2|        3|       1506|   11816|     1|         0|  25|\n",
      "|e81aacfe-17fe-46b...|2020-06-16|      2016|181925|       3|        3|       2137|   11709|     2|         0|  22|\n",
      "|2ed8d509-7372-46d...|2021-08-06|      2015|258710|       3|        3|       1918|    9666|     1|         0|  25|\n",
      "|f876d86f-3c9f-42b...|2019-02-27|      2011|167864|       3|        3|       2471|   13924|     2|         0|  15|\n",
      "|0a2bd445-8508-4d8...|2021-12-30|      2014|337527|       2|        3|       1926|   12556|     1|         0|  23|\n",
      "|941bad30-eb49-4a7...|2020-05-09|      2015|229896|       3|        3|       2197|    8641|     1|         0|   3|\n",
      "|dd61eb34-6589-4c0...|2021-07-25|      2016|210247|       3|        2|       1672|   11986|     2|         0|  28|\n",
      "|f1e4cef7-d151-439...|2019-02-01|      2011|398667|       2|        3|       2331|   11356|     1|         0|   7|\n",
      "|ea620c7b-c2f7-4c6...|2021-05-31|      2011|437958|       3|        3|       2356|   11052|     1|         0|  26|\n",
      "|f233cb41-6f33-4b0...|2021-07-18|      2016|437375|       4|        3|       1704|   11721|     2|         0|  34|\n",
      "|c797ca12-52cd-4b1...|2019-06-08|      2015|288650|       2|        3|       2100|   10419|     2|         0|   7|\n",
      "|0cfe57f3-28c2-472...|2019-10-04|      2015|308313|       3|        3|       1960|    9453|     2|         0|   2|\n",
      "|4566cd2a-ac6e-435...|2019-07-15|      2016|177541|       3|        3|       2130|   10517|     2|         0|  25|\n",
      "+--------------------+----------+----------+------+--------+---------+-----------+--------+------+----------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a temporary table or view\n",
    "home_df.createOrReplaceTempView(\"home_sales\")\n",
    "\n",
    "# Now, the the temporary table can be used for  SparkSQL queries\n",
    "query_all = spark.sql(\"SELECT * FROM home_sales\")\n",
    "\n",
    "# Preview the Temporary Table containing the Home Sales Dataset\n",
    "query_all.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step #4: Perform SparkSQL Queries\n",
    "---\n",
    "### Query #1: What is the average price for a four-bedroom house sold for each year? (Round to 2 d.p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query #1 Runtime: 0.1874985694885254 seconds\n",
      "+---------+---------+\n",
      "|year_sold|avg_price|\n",
      "+---------+---------+\n",
      "|     2019| 300263.7|\n",
      "|     2020|298353.78|\n",
      "|     2021|301819.44|\n",
      "|     2022|296363.88|\n",
      "+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "query_one = spark.sql(\"\"\"\n",
    "                    SELECT YEAR(date) AS year_sold, ROUND(AVG(price), 2) AS avg_price\n",
    "                    FROM home_sales\n",
    "                    WHERE bedrooms = 4\n",
    "                    GROUP BY year_sold\n",
    "                    ORDER BY year_sold\n",
    "                    \"\"\"\n",
    "                    )\n",
    "\n",
    "print(\"Query #1 Runtime: %s seconds\" % (time.time() - start_time))\n",
    "query_one.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query #2: What is the average price of a home for each year it was built that has three bedrooms and three bathrooms? (Round to 2 d.p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query #2 Runtime: 0.13862943649291992 seconds\n",
      "+----------+---------+\n",
      "|year_built|avg_price|\n",
      "+----------+---------+\n",
      "|      2010|292859.62|\n",
      "|      2011|291117.47|\n",
      "|      2012|293683.19|\n",
      "|      2013|295962.27|\n",
      "|      2014|290852.27|\n",
      "|      2015| 288770.3|\n",
      "|      2016|290555.07|\n",
      "|      2017|292676.79|\n",
      "+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "query_two = spark.sql(\"\"\"\n",
    "                    SELECT date_built AS year_built, ROUND(AVG(price), 2) AS avg_price\n",
    "                    FROM home_sales\n",
    "                    WHERE bedrooms = 3 AND bathrooms = 3\n",
    "                    GROUP BY date_built\n",
    "                    ORDER BY date_built\n",
    "                    \"\"\"\n",
    "                    )\n",
    "\n",
    "print(\"Query #2 Runtime: %s seconds\" % (time.time() - start_time))\n",
    "query_two.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query #3: What is the average price of a home for each year that has three bedrooms, three bathrooms, two floors, and is greater than or equal to 2,000 square feet? (Round to 2 d.p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query #3 Runtime: 0.039893388748168945 seconds\n",
      "+----------+---------+\n",
      "|year_built|avg_price|\n",
      "+----------+---------+\n",
      "|      2010|285010.22|\n",
      "|      2011|276553.81|\n",
      "|      2012|307539.97|\n",
      "|      2013|303676.79|\n",
      "|      2014|298264.72|\n",
      "|      2015|297609.97|\n",
      "|      2016| 293965.1|\n",
      "|      2017|280317.58|\n",
      "+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "query_three = spark.sql(\"\"\"\n",
    "                        SELECT date_built AS year_built, ROUND(AVG(price), 2) AS avg_price\n",
    "                        FROM home_sales\n",
    "                        WHERE bedrooms = 3 AND bathrooms = 3 AND floors = 2 AND sqft_living >= 2000\n",
    "                        GROUP BY date_built\n",
    "                        ORDER BY date_built\n",
    "                        \"\"\"\n",
    "                        )\n",
    "\n",
    "print(\"Query #3 Runtime: %s seconds\" % (time.time() - start_time))\n",
    "query_three.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query #4: What is the \"view\" rating for the average price of homes costing more than or equal to $350,000? (Round to 2 d.p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query #4 (Uncached) Runtime: 0.039893150329589844 seconds\n",
      "+-----------+---------+\n",
      "|view_rating|avg_price|\n",
      "+-----------+---------+\n",
      "|          0|403848.51|\n",
      "|          1|401044.25|\n",
      "|          2|397389.25|\n",
      "|          3| 398867.6|\n",
      "|          4|399631.89|\n",
      "|          5|401471.82|\n",
      "|          6|395655.38|\n",
      "|          7|403005.77|\n",
      "|          8|398592.71|\n",
      "|          9|401393.34|\n",
      "|         10|401868.43|\n",
      "|         11|399548.12|\n",
      "|         12|401501.32|\n",
      "|         13|398917.98|\n",
      "|         14|398570.03|\n",
      "|         15| 404673.3|\n",
      "|         16|399586.53|\n",
      "|         17|398474.49|\n",
      "|         18|399332.91|\n",
      "|         19|398953.17|\n",
      "+-----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "query_four = spark.sql(\"\"\"\n",
    "                       SELECT CAST(view AS INT) AS view_rating, ROUND(AVG(price), 2) AS avg_price\n",
    "                       FROM home_sales\n",
    "                       WHERE price >= 350000\n",
    "                       GROUP BY view_rating\n",
    "                       ORDER BY view_rating ASC\n",
    "                       \"\"\"\n",
    "                       )\n",
    "\n",
    "runtime_query_four = time.time() - start_time\n",
    "\n",
    "print(\"Query #4 (Uncached) Runtime: %s seconds\" % (runtime_query_four))\n",
    "query_four.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step #5: Cache the Temporary Table `home_sales`\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.cacheTable(\"home_sales\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step #6: Confirm that the Temporary Table `home_sales` is Cached\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.isCached('home_sales')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step #7: Using the Cached Data, Run a SparkSQL Query that Filters Out the View Ratings with an Average Price of Greater than or Equal to $350,000. Determine the Runtime and Compare it to the Uncached Runtime\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query #5 (Cached) Runtime: 0.030916929244995117 seconds\n",
      "\n",
      "The Runtime for the Cached Query is FASTER than the Uncached Query\n",
      "\n",
      "+-----------+---------+\n",
      "|view_rating|avg_price|\n",
      "+-----------+---------+\n",
      "|          0|403848.51|\n",
      "|          1|401044.25|\n",
      "|          2|397389.25|\n",
      "|          3| 398867.6|\n",
      "|          4|399631.89|\n",
      "|          5|401471.82|\n",
      "|          6|395655.38|\n",
      "|          7|403005.77|\n",
      "|          8|398592.71|\n",
      "|          9|401393.34|\n",
      "|         10|401868.43|\n",
      "|         11|399548.12|\n",
      "|         12|401501.32|\n",
      "|         13|398917.98|\n",
      "|         14|398570.03|\n",
      "|         15| 404673.3|\n",
      "|         16|399586.53|\n",
      "|         17|398474.49|\n",
      "|         18|399332.91|\n",
      "|         19|398953.17|\n",
      "+-----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "query_five = spark.sql(\"\"\"\n",
    "                       SELECT CAST(view AS INT) AS view_rating, ROUND(AVG(price), 2) AS avg_price\n",
    "                       FROM home_sales\n",
    "                       WHERE price >= 350000\n",
    "                       GROUP BY view_rating\n",
    "                       ORDER BY view_rating ASC\n",
    "                       \"\"\"\n",
    "                       )\n",
    "\n",
    "runtime_query_five = time.time() - start_time\n",
    "\n",
    "print(\"Query #5 (Cached) Runtime: %s seconds\" % (runtime_query_five))\n",
    "print()\n",
    "\n",
    "if (runtime_query_five < runtime_query_four) :\n",
    "    print(\"The Runtime for the Cached Query is FASTER than the Uncached Query\")\n",
    "    print()\n",
    "else :\n",
    "    print(\"The Runtime for the Uncached Query is FASTER than the Cached Query\")\n",
    "    print()\n",
    "\n",
    "query_five.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step #8: Partition by the `date_built` Field on the Formatted Parquet Home Sales Data (`parquet_df`)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_output_directory = \"data_partitioned_by_date_built\"\n",
    "\n",
    "home_df.write.partitionBy(\"date_built\").mode(\"overwrite\").parquet(parquet_output_directory)\n",
    "\n",
    "# Read the Formatted Paraquet Data\n",
    "parquet_df = spark.read.parquet(parquet_output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step #9: Create a Temporary Table for the Parquet Data (`parquet_home_sales`)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_df.createOrReplaceTempView(\"parquet_home_sales\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step #10: Using the Parquet Temporary Table (`parquet_home_sales`), Run a SparkSQL Query that Filters Out the View Ratings with an Average Price of Greater than or Equal to $350,000. Determine the Runtime and Compare it to the Uncached Runtime\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query #6 (Parquet) Runtime: 0.01795196533203125 seconds\n",
      "\n",
      "The Runtime for the Parquet Query is FASTER than the Uncached Query\n",
      "\n",
      "+-----------+---------+\n",
      "|view_rating|avg_price|\n",
      "+-----------+---------+\n",
      "|          0|403848.51|\n",
      "|          1|401044.25|\n",
      "|          2|397389.25|\n",
      "|          3| 398867.6|\n",
      "|          4|399631.89|\n",
      "|          5|401471.82|\n",
      "|          6|395655.38|\n",
      "|          7|403005.77|\n",
      "|          8|398592.71|\n",
      "|          9|401393.34|\n",
      "|         10|401868.43|\n",
      "|         11|399548.12|\n",
      "|         12|401501.32|\n",
      "|         13|398917.98|\n",
      "|         14|398570.03|\n",
      "|         15| 404673.3|\n",
      "|         16|399586.53|\n",
      "|         17|398474.49|\n",
      "|         18|399332.91|\n",
      "|         19|398953.17|\n",
      "+-----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "query_six = spark.sql(\"\"\"\n",
    "                      SELECT CAST(view AS INT) AS view_rating, ROUND(AVG(price), 2) AS avg_price\n",
    "                      FROM parquet_home_sales\n",
    "                      WHERE price >= 350000\n",
    "                      GROUP BY view_rating\n",
    "                      ORDER BY view_rating ASC\n",
    "                      \"\"\"\n",
    "                      )\n",
    "\n",
    "runtime_query_six = time.time() - start_time\n",
    "\n",
    "print(\"Query #6 (Parquet) Runtime: %s seconds\" % (runtime_query_six))\n",
    "print()\n",
    "\n",
    "if (runtime_query_six < runtime_query_four) :\n",
    "    print(\"The Runtime for the Parquet Query is FASTER than the Uncached Query\")\n",
    "    print()\n",
    "else :\n",
    "    print(\"The Runtime for the Uncached Query is FASTER than the Parquet Query\")\n",
    "    print()\n",
    "\n",
    "query_six.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step #11: Uncache the `home_sales` Temporary Table\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.uncacheTable('home_sales')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step #12: Verify the `home_sales` Temporary Table is Uncached\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.isCached('home_sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Terminate the Spark session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
